{
  "name": "AI Product Factory - Context Scavenging",
  "nodes": [
    {
      "parameters": {},
      "id": "error-trigger",
      "name": "Error Trigger",
      "type": "n8n-nodes-base.errorTrigger",
      "typeVersion": 1,
      "position": [-200, 400]
    },
    {
      "parameters": {
        "jsCode": "// Error Handler - Log workflow errors for centralized handling\n// This node captures any workflow errors and formats them for logging\n\nconst error = $input.first().json;\n\nconst errorDetails = {\n  timestamp: new Date().toISOString(),\n  workflow: 'AI Product Factory - Context Scavenging',\n  error_type: error.execution?.error?.name || 'UnknownError',\n  error_message: error.execution?.error?.message || 'No error message available',\n  error_stack: error.execution?.error?.stack || '',\n  execution_id: error.execution?.id || 'unknown',\n  node_name: error.execution?.lastNodeExecuted || 'unknown',\n  mode: error.execution?.mode || 'unknown',\n  retry_of: error.execution?.retryOf || null,\n  severity: 'ERROR',\n  recoverable: false\n};\n\n// Log to console for n8n logs\nconsole.error('Scavenging Error:', JSON.stringify(errorDetails, null, 2));\n\nreturn {\n  json: errorDetails\n};"
      },
      "id": "error-handler",
      "name": "Error Handler",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [0, 400]
    },
    {
      "parameters": {
        "workflowInputs": {
          "values": [
            {
              "name": "project_id",
              "type": "string"
            },
            {
              "name": "session_id",
              "type": "string"
            },
            {
              "name": "input_files",
              "type": "string"
            }
          ]
        }
      },
      "id": "subworkflow-entry",
      "name": "Subworkflow Entry Point",
      "type": "n8n-nodes-base.executeWorkflowTrigger",
      "typeVersion": 1.1,
      "position": [0, 0]
    },
    {
      "parameters": {
        "jsCode": "// Context Scavenging - State Initializer\n// Validates input and initializes scavenging state (S3-only, no Google Drive)\n\nconst input = $input.first().json;\n\n// Validate required fields\nif (!input.project_id) {\n  throw new Error('Missing required field: project_id');\n}\n\nif (!input.session_id) {\n  throw new Error('Missing required field: session_id');\n}\n\n// Parse input files (from S3)\nlet inputFiles = [];\nif (input.input_files) {\n  if (typeof input.input_files === 'string') {\n    try {\n      inputFiles = JSON.parse(input.input_files);\n    } catch (e) {\n      inputFiles = [];\n    }\n  } else if (Array.isArray(input.input_files)) {\n    inputFiles = input.input_files;\n  }\n}\n\nif (inputFiles.length === 0) {\n  throw new Error('No input files provided. Upload documents to S3 first.');\n}\n\n// Generate unique scavenging session ID\nconst scavenging_id = 'scav_' + Date.now() + '_' + Math.random().toString(36).substring(2, 8);\n\nreturn {\n  json: {\n    project_id: input.project_id,\n    session_id: input.session_id,\n    scavenging_id,\n    input_files: inputFiles,\n    total_files: inputFiles.length,\n    documents_processed: 0,\n    standards_found: [],\n    start_time: new Date().toISOString(),\n    status: 'initializing'\n  }\n};"
      },
      "id": "state-initializer",
      "name": "State Initializer",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [220, 0]
    },
    {
      "parameters": {
        "jsCode": "// Prepare documents from S3 input files for processing\n\nconst state = $input.first().json;\nconst inputFiles = state.input_files || [];\n\n// Map input files to document format\nconst documents = inputFiles.map((file, index) => ({\n  id: `s3_${index}_${Date.now()}`,\n  key: file.key,\n  name: file.name,\n  size: file.size,\n  contentType: file.contentType,\n  source: 's3',\n  uploadedAt: file.uploadedAt\n}));\n\nreturn {\n  json: {\n    ...state,\n    documents,\n    total_documents: documents.length,\n    status: 'ready_for_processing'\n  }\n};"
      },
      "id": "prepare-documents",
      "name": "Prepare Documents",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [440, 0]
    },
    {
      "parameters": {
        "batchSize": 1,
        "options": {
          "reset": false
        }
      },
      "id": "batch-processor",
      "name": "Document Batch Processor",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [660, 0]
    },
    {
      "parameters": {
        "jsCode": "// Prepare document for S3 download\n\nconst state = $('Prepare Documents').item.json;\nconst batchItem = $input.first().json;\n\n// Find the current document in the documents array\nconst documents = state.documents || [];\nconst currentIndex = $('Document Batch Processor').context.currentRunIndex || 0;\nconst document = documents[currentIndex] || batchItem;\n\nreturn {\n  json: {\n    ...state,\n    current_document: document,\n    current_index: currentIndex\n  }\n};"
      },
      "id": "prepare-single-doc",
      "name": "Prepare Single Document",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [880, 0]
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "mode": "name",
          "value": "AI Product Factory - S3 Storage Operations"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "operation": "download_artifact",
            "key": "={{ $json.current_document.key }}"
          }
        }
      },
      "id": "download-from-s3",
      "name": "Download from S3",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [1100, 0]
    },
    {
      "parameters": {
        "jsCode": "// Extract text content from downloaded S3 file\n\nconst prevState = $('Prepare Single Document').item.json;\nconst s3Response = $input.first().json;\n\nlet textContent = '';\n\n// Extract content from S3 response\nif (s3Response.content) {\n  textContent = s3Response.content;\n} else if (s3Response.data) {\n  // Handle base64 encoded content\n  if (typeof s3Response.data === 'string') {\n    try {\n      textContent = Buffer.from(s3Response.data, 'base64').toString('utf8');\n    } catch (e) {\n      textContent = s3Response.data;\n    }\n  } else {\n    textContent = JSON.stringify(s3Response.data);\n  }\n}\n\n// Handle PDF content (may need special parsing)\nconst contentType = prevState.current_document.contentType || '';\nif (contentType === 'application/pdf' && !textContent) {\n  textContent = '[PDF content - text extraction may be limited]';\n}\n\n// Limit content length to prevent token overflow\nconst maxLength = 15000;\nif (textContent.length > maxLength) {\n  textContent = textContent.substring(0, maxLength) + '\\n\\n[Content truncated...]';\n}\n\n// Store in static data for later retrieval\nconst staticData = $getWorkflowStaticData('global');\nstaticData.currentDocumentState = {\n  project_id: prevState.project_id,\n  session_id: prevState.session_id,\n  scavenging_id: prevState.scavenging_id,\n  current_document: prevState.current_document,\n  document_content: textContent\n};\n\nreturn {\n  json: {\n    ...prevState,\n    document_content: textContent,\n    content_length: textContent.length\n  }\n};"
      },
      "id": "extract-content",
      "name": "Extract Text Content",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1320, 0]
    },
    {
      "parameters": {
        "options": {
          "systemMessage": "## Context\nYou are the Technical Scavenger - a meticulous analyst who extracts technical decisions and constraints from documentation with forensic precision.\n\n## Objective\nExtract ALL technical standards, patterns, decisions, and constraints from the provided document. Focus on:\n- Technology choices (databases, frameworks, languages, libraries)\n- Architecture patterns (microservices, event-driven, monolith, serverless)\n- Security requirements (authentication methods, encryption standards)\n- Integration constraints (APIs, protocols, data formats)\n- Compliance requirements (GDPR, SOC2, HIPAA, PCI-DSS)\n- Infrastructure decisions (cloud providers, hosting, containerization)\n- Development practices (testing strategies, CI/CD, coding standards)\n\n## Style\nOutput as a structured JSON array. Be thorough but avoid hallucination.\nIf uncertain about something, mark confidence as low.\nOnly extract what is explicitly stated or strongly implied.\n\n## Tone\nProfessional, analytical, precise.\n\n## Audience\nTechnical architects and decision-makers.\n\n## Response Format\nYou MUST respond with ONLY a valid JSON array. No explanation, no markdown, just the JSON:\n```json\n[\n  {\n    \"name\": \"Technology/Pattern Name\",\n    \"type\": \"technology|pattern|standard|requirement|constraint\",\n    \"description\": \"Detailed description of what this entails\",\n    \"source\": \"Document name or section where found\",\n    \"confidence\": 0.0-1.0,\n    \"category\": \"database|framework|language|security|infrastructure|integration|compliance|development\"\n  }\n]\n```\n\nIf no technical standards are found, return an empty array: []"
        },
        "text": "=Document: {{ $json.current_document.name }}\n\n---\n\n{{ $json.document_content }}\n\n---\n\nExtract all technical standards, patterns, and constraints from this document. Return ONLY a JSON array."
      },
      "id": "scavenger-agent",
      "name": "Scavenger Agent",
      "type": "@n8n/n8n-nodes-langchain.agent",
      "typeVersion": 1.7,
      "position": [1540, 0]
    },
    {
      "parameters": {
        "model": "={{ $env.MODEL_CONTEXT || 'google/gemini-1.5-pro' }}",
        "options": {
          "temperature": 0.2,
          "maxTokens": 4096
        }
      },
      "id": "scavenger-model",
      "name": "Claude 3.5 Sonnet",
      "type": "@n8n/n8n-nodes-langchain.lmChatOpenRouter",
      "typeVersion": 1,
      "position": [1540, 220],
      "credentials": {
        "openRouterApi": {
          "id": "openrouter-api",
          "name": "OpenRouter API"
        }
      }
    },
    {
      "parameters": {
        "jsCode": "// Parse Scavenger Agent response and extract standards\n// Accumulate standards across all document iterations using static data\n\nconst agentResponse = $input.first().json;\nconst responseText = agentResponse.output || agentResponse.text || '';\n\n// Get the previous state from static data\nconst staticData = $getWorkflowStaticData('global');\nconst prevState = staticData.currentDocumentState || {};\n\n// Try to parse JSON from response\nlet extractedStandards = [];\ntry {\n  extractedStandards = JSON.parse(responseText);\n} catch (e) {\n  // Try to extract JSON from markdown code block\n  const jsonMatch = responseText.match(/```(?:json)?\\s*([\\s\\S]*?)```/);\n  if (jsonMatch) {\n    try {\n      extractedStandards = JSON.parse(jsonMatch[1].trim());\n    } catch (e2) {\n      const arrayMatch = responseText.match(/\\[\\s*\\{[\\s\\S]*\\}\\s*\\]/);\n      if (arrayMatch) {\n        try {\n          extractedStandards = JSON.parse(arrayMatch[0]);\n        } catch (e3) {\n          extractedStandards = [];\n        }\n      }\n    }\n  }\n}\n\n// Ensure it's an array\nif (!Array.isArray(extractedStandards)) {\n  extractedStandards = extractedStandards ? [extractedStandards] : [];\n}\n\n// Add document source and unique ID to each standard\nconst currentDoc = prevState.current_document || { name: 'unknown', key: 'unknown' };\nconst standardsWithSource = extractedStandards.map((std, idx) => ({\n  id: `tech_${Date.now()}_${idx}_${Math.random().toString(36).substring(2, 6)}`,\n  ...std,\n  document_name: currentDoc.name,\n  document_key: currentDoc.key,\n  extracted_at: new Date().toISOString()\n}));\n\n// Accumulate standards in workflow static data\nif (!staticData.accumulatedStandards) {\n  staticData.accumulatedStandards = [];\n}\nstaticData.accumulatedStandards.push(...standardsWithSource);\nstaticData.lastProcessedState = {\n  project_id: prevState.project_id || 'unknown',\n  session_id: prevState.session_id || 'unknown',\n  scavenging_id: prevState.scavenging_id || 'unknown'\n};\n\nreturn {\n  json: {\n    project_id: prevState.project_id || 'unknown',\n    session_id: prevState.session_id || 'unknown',\n    scavenging_id: prevState.scavenging_id || 'unknown',\n    document_name: currentDoc.name,\n    standards_count: standardsWithSource.length,\n    standards: standardsWithSource,\n    accumulated_count: staticData.accumulatedStandards.length\n  }\n};"
      },
      "id": "parse-standards",
      "name": "Parse Extracted Standards",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [1760, 0]
    },
    {
      "parameters": {
        "jsCode": "// Aggregate all extracted standards from static data\n// Deduplicate and prepare for Perplexity enrichment\n\nconst input = $input.first().json;\n\n// Read accumulated standards from workflow static data\nconst staticData = $getWorkflowStaticData('global');\nconst allStandards = staticData.accumulatedStandards || [];\nconst lastState = staticData.lastProcessedState || {};\n\n// CLEANUP: Clear static data to prevent state leakage\ndelete staticData.accumulatedStandards;\ndelete staticData.lastProcessedState;\ndelete staticData.currentDocumentState;\n\n// Deduplicate by name (keep highest confidence)\nconst standardsMap = new Map();\nallStandards.forEach(std => {\n  const key = (std.name || '').toLowerCase();\n  if (!standardsMap.has(key) || (std.confidence || 0) > (standardsMap.get(key).confidence || 0)) {\n    standardsMap.set(key, std);\n  }\n});\n\nconst uniqueStandards = Array.from(standardsMap.values());\n\nconst project_id = lastState.project_id || input.project_id || 'unknown';\nconst session_id = lastState.session_id || input.session_id || 'unknown';\nconst scavenging_id = lastState.scavenging_id || input.scavenging_id || 'unknown';\n\nreturn {\n  json: {\n    project_id,\n    session_id,\n    scavenging_id,\n    total_standards: uniqueStandards.length,\n    standards: uniqueStandards,\n    status: uniqueStandards.length > 0 ? 'standards_extracted' : 'no_standards_found'\n  }\n};"
      },
      "id": "aggregate-standards",
      "name": "Aggregate All Standards",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2000, 0]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "has-standards",
              "leftValue": "={{ $json.total_standards }}",
              "rightValue": 0,
              "operator": {
                "type": "number",
                "operation": "gt"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "id": "has-standards-check",
      "name": "Has Standards?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [2220, 0]
    },
    {
      "parameters": {
        "jsCode": "// No standards found - return early\n\nconst state = $input.first().json;\n\nreturn {\n  json: {\n    project_id: state.project_id,\n    session_id: state.session_id,\n    scavenging_id: state.scavenging_id,\n    phase: 0,\n    status: 'completed',\n    summary: {\n      total_standards_found: 0,\n      approved_global: 0,\n      approved_local: 0,\n      skipped: 0\n    },\n    processed_standards: [],\n    message: 'No technical standards found in uploaded documents',\n    completed_at: new Date().toISOString()\n  }\n};"
      },
      "id": "no-standards-handler",
      "name": "No Standards Handler",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2460, 200]
    },
    {
      "parameters": {
        "jsCode": "// Split standards for Perplexity enrichment\n// Each standard gets 3 alternatives researched\n\nconst state = $input.first().json;\nconst standards = state.standards || [];\n\n// Return each standard as separate item for parallel Perplexity calls\nreturn standards.map((std, index) => ({\n  json: {\n    project_id: state.project_id,\n    session_id: state.session_id,\n    scavenging_id: state.scavenging_id,\n    standard_index: index,\n    total_standards: standards.length,\n    standard: std\n  }\n}));"
      },
      "id": "split-for-perplexity",
      "name": "Split for Perplexity",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [2460, -100]
    },
    {
      "parameters": {
        "batchSize": 3,
        "options": {
          "reset": false
        }
      },
      "id": "perplexity-batch",
      "name": "Perplexity Batch (3 parallel)",
      "type": "n8n-nodes-base.splitInBatches",
      "typeVersion": 3,
      "position": [2680, -100]
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "mode": "name",
          "value": "AI Product Factory - Perplexity Research"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "research_type": "best_practices",
            "query": "={{ 'top 3 alternatives to ' + $json.standard.name + ' for ' + ($json.standard.category || 'software development') + '. List name, brief description, pros and cons for each.' }}",
            "max_results": "3"
          }
        }
      },
      "id": "perplexity-research",
      "name": "Fetch Alternatives (Perplexity)",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [2900, -100]
    },
    {
      "parameters": {
        "jsCode": "// Parse Perplexity response and extract alternatives\n\nconst prevData = $('Perplexity Batch (3 parallel)').item.json;\nconst perplexityResult = $input.first().json;\n\n// Parse alternatives from Perplexity response\nlet alternatives = [];\nconst responseText = perplexityResult.result || perplexityResult.output || perplexityResult.text || '';\n\n// Try to extract structured alternatives\ntry {\n  // Look for JSON in response\n  const jsonMatch = responseText.match(/```(?:json)?\\s*([\\s\\S]*?)```/);\n  if (jsonMatch) {\n    alternatives = JSON.parse(jsonMatch[1].trim());\n  } else {\n    // Parse numbered list format\n    const lines = responseText.split('\\n');\n    let currentAlt = null;\n    \n    for (const line of lines) {\n      const nameMatch = line.match(/^\\d+\\.\\s*\\*?\\*?([^:*]+)\\*?\\*?[:\\s]/);\n      if (nameMatch) {\n        if (currentAlt) alternatives.push(currentAlt);\n        currentAlt = {\n          name: nameMatch[1].trim(),\n          description: '',\n          pros: [],\n          cons: []\n        };\n      } else if (currentAlt) {\n        const prosMatch = line.match(/pros?[:\\s]+(.+)/i);\n        const consMatch = line.match(/cons?[:\\s]+(.+)/i);\n        const descMatch = line.match(/description[:\\s]+(.+)/i);\n        \n        if (prosMatch) {\n          currentAlt.pros = prosMatch[1].split(',').map(p => p.trim());\n        } else if (consMatch) {\n          currentAlt.cons = consMatch[1].split(',').map(c => c.trim());\n        } else if (descMatch) {\n          currentAlt.description = descMatch[1].trim();\n        } else if (!currentAlt.description && line.trim()) {\n          currentAlt.description = line.trim();\n        }\n      }\n    }\n    if (currentAlt) alternatives.push(currentAlt);\n  }\n} catch (e) {\n  // Fallback: create minimal alternatives from text\n  const nameMatches = responseText.match(/\\*\\*([^*]+)\\*\\*/g) || [];\n  alternatives = nameMatches.slice(0, 3).map(match => ({\n    name: match.replace(/\\*/g, '').trim(),\n    description: 'Alternative technology option',\n    pros: [],\n    cons: []\n  }));\n}\n\n// Ensure we have valid array with max 3 items\nif (!Array.isArray(alternatives)) alternatives = [];\nalternatives = alternatives.slice(0, 3);\n\n// Add alternatives to the standard\nconst enrichedStandard = {\n  ...prevData.standard,\n  alternatives\n};\n\nreturn {\n  json: {\n    ...prevData,\n    standard: enrichedStandard,\n    alternatives_count: alternatives.length\n  }\n};"
      },
      "id": "parse-alternatives",
      "name": "Parse Alternatives",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [3120, -100]
    },
    {
      "parameters": {
        "jsCode": "// Aggregate all enriched standards after Perplexity batch completes\n\nconst allItems = $input.all();\n\n// Get common data from first item\nconst firstItem = allItems[0]?.json || {};\n\n// Collect all enriched standards\nconst enrichedStandards = allItems.map(item => item.json.standard);\n\nreturn {\n  json: {\n    project_id: firstItem.project_id,\n    session_id: firstItem.session_id,\n    scavenging_id: firstItem.scavenging_id,\n    detected_stack: enrichedStandards,\n    total_standards: enrichedStandards.length\n  }\n};"
      },
      "id": "aggregate-enriched",
      "name": "Aggregate Enriched Standards",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [3360, -100]
    },
    {
      "parameters": {
        "jsCode": "// Build Governance Payload for batch UI approval\n// This creates the JSON structure expected by the GovernanceWidget\n\nconst state = $input.first().json;\n\n// Get webhook base URL from environment\nconst webhookBaseUrl = $env.N8N_WEBHOOK_URL || 'http://localhost:5678/webhook';\nconst webhookUrl = `${webhookBaseUrl}/governance-batch-${state.scavenging_id}`;\n\n// Build the governance payload matching GovernancePayloadSchema\nconst governancePayload = {\n  type: 'governance_request',\n  scavenging_id: state.scavenging_id,\n  project_id: state.project_id,\n  detected_stack: state.detected_stack.map(tech => ({\n    id: tech.id,\n    name: tech.name,\n    type: tech.type || 'technology',\n    category: tech.category || 'development',\n    description: tech.description || '',\n    source: tech.document_name || tech.source || 'uploaded document',\n    confidence: tech.confidence || 0.8,\n    alternatives: (tech.alternatives || []).map(alt => ({\n      name: alt.name,\n      description: alt.description || '',\n      pros: alt.pros || [],\n      cons: alt.cons || []\n    }))\n  })),\n  webhook_url: webhookUrl\n};\n\n// Chat message to send to frontend\nconst chatMessage = {\n  role: 'assistant',\n  content: `I've analyzed your uploaded documents and detected ${state.detected_stack.length} technology standards. Please review and approve them using the Tech Stack Configurator below.`,\n  message_type: 'governance_request',\n  payload: governancePayload\n};\n\nreturn {\n  json: {\n    ...state,\n    governance_payload: governancePayload,\n    chat_message: chatMessage,\n    webhook_suffix: `governance-batch-${state.scavenging_id}`\n  }\n};"
      },
      "id": "build-governance-payload",
      "name": "Build Governance Payload",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [3580, -100]
    },
    {
      "parameters": {
        "method": "POST",
        "url": "={{ $env.DASHBOARD_WEBHOOK_URL || 'http://dashboard:3000/api/chat/message' }}",
        "sendHeaders": true,
        "headerParameters": {
          "parameters": [
            {
              "name": "Content-Type",
              "value": "application/json"
            }
          ]
        },
        "sendBody": true,
        "specifyBody": "json",
        "jsonBody": "={{ JSON.stringify({ project_id: $json.project_id, session_id: $json.session_id, message: $json.chat_message }) }}"
      },
      "id": "send-governance-message",
      "name": "Send Governance Message to Dashboard",
      "type": "n8n-nodes-base.httpRequest",
      "typeVersion": 4.2,
      "position": [3800, -100]
    },
    {
      "parameters": {
        "resume": "webhook",
        "options": {
          "webhookSuffix": "={{ $('Build Governance Payload').item.json.webhook_suffix }}"
        }
      },
      "id": "wait-for-batch-approval",
      "name": "Wait for Batch Governance Response",
      "type": "n8n-nodes-base.wait",
      "typeVersion": 1.1,
      "position": [4020, -100],
      "webhookId": "governance-batch-approval"
    },
    {
      "parameters": {
        "jsCode": "// Process batch governance response from frontend GovernanceWidget\n\nconst prevState = $('Build Governance Payload').item.json;\nconst webhookData = $input.first().json;\n\n// Parse the governance response\n// Expected format: { scavenging_id, project_id, decisions: [{ tech_id, action, scope, selected_alternative }] }\nconst response = webhookData.body || webhookData;\n\nconst decisions = response.decisions || [];\n\n// Match decisions back to detected stack\nconst processedStandards = prevState.detected_stack.map(tech => {\n  const decision = decisions.find(d => d.tech_id === tech.id) || { action: 'skip' };\n  \n  return {\n    ...tech,\n    user_decision: decision.action,\n    scope: decision.scope || null,\n    selected_alternative: decision.selected_alternative || null,\n    approved: decision.action === 'approve',\n    response_timestamp: new Date().toISOString()\n  };\n});\n\n// Separate approved and skipped\nconst approvedStandards = processedStandards.filter(s => s.approved);\nconst skippedStandards = processedStandards.filter(s => !s.approved);\n\nreturn {\n  json: {\n    project_id: prevState.project_id,\n    session_id: prevState.session_id,\n    scavenging_id: prevState.scavenging_id,\n    processed_standards: processedStandards,\n    approved_standards: approvedStandards,\n    skipped_standards: skippedStandards,\n    total_processed: processedStandards.length,\n    total_approved: approvedStandards.length,\n    total_skipped: skippedStandards.length\n  }\n};"
      },
      "id": "process-batch-response",
      "name": "Process Batch Response",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [4240, -100]
    },
    {
      "parameters": {
        "jsCode": "// Split approved standards for storage\n\nconst state = $input.first().json;\nconst approved = state.approved_standards || [];\n\nif (approved.length === 0) {\n  // No standards to store - return final summary\n  return {\n    json: {\n      ...state,\n      storage_skipped: true\n    }\n  };\n}\n\n// Return each approved standard for storage\nreturn approved.map(std => ({\n  json: {\n    ...state,\n    current_standard: std\n  }\n}));"
      },
      "id": "split-for-storage",
      "name": "Split Approved for Storage",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [4460, -100]
    },
    {
      "parameters": {
        "conditions": {
          "options": {
            "caseSensitive": true,
            "leftValue": "",
            "typeValidation": "strict"
          },
          "conditions": [
            {
              "id": "has-approved",
              "leftValue": "={{ $json.storage_skipped }}",
              "rightValue": true,
              "operator": {
                "type": "boolean",
                "operation": "notEquals"
              }
            }
          ],
          "combinator": "and"
        }
      },
      "id": "has-approved-check",
      "name": "Has Approved Standards?",
      "type": "n8n-nodes-base.if",
      "typeVersion": 2.2,
      "position": [4680, -100]
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "mode": "name",
          "value": "Titan - Graphiti Operations"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "operation": "add_episode",
            "name": "={{ 'Tech Standard: ' + $json.current_standard.name }}",
            "content": "={{ JSON.stringify($json.current_standard) }}",
            "source": "document",
            "source_description": "={{ 'Extracted from: ' + $json.current_standard.document_name }}",
            "group_id": "={{ $json.current_standard.scope === 'global' ? 'global_standards' : $json.project_id }}"
          }
        }
      },
      "id": "store-in-graphiti",
      "name": "Store in Graphiti",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [4920, -200]
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "mode": "name",
          "value": "Titan - Qdrant Operations"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "operation": "upsert",
            "content": "={{ $json.current_standard.name + ': ' + $json.current_standard.description }}",
            "scope": "={{ $json.current_standard.scope }}",
            "type": "standard",
            "source": "={{ $json.current_standard.document_name }}",
            "metadata": "={{ JSON.stringify({ category: $json.current_standard.category, confidence: $json.current_standard.confidence, type: $json.current_standard.type, selected_alternative: $json.current_standard.selected_alternative }) }}"
          }
        }
      },
      "id": "store-in-qdrant",
      "name": "Store in Qdrant",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [4920, 0]
    },
    {
      "parameters": {
        "workflowId": {
          "__rl": true,
          "mode": "name",
          "value": "AI Product Factory - Decision Logger"
        },
        "workflowInputs": {
          "mappingMode": "defineBelow",
          "value": {
            "operation": "log_decision",
            "project_id": "={{ $json.project_id }}",
            "session_id": "={{ $json.session_id }}",
            "phase": "0",
            "entry_type": "tech_standard_approval",
            "content": "={{ $json.current_standard.name }}",
            "metadata": "={{ JSON.stringify({ name: $json.current_standard.name, type: $json.current_standard.type, source: $json.current_standard.document_name, confidence: $json.current_standard.confidence, scope: $json.current_standard.scope, selected_alternative: $json.current_standard.selected_alternative, stored_in: 'Graphiti (' + ($json.current_standard.scope === 'global' ? 'global_standards' : $json.project_id) + '), Qdrant (scope: ' + $json.current_standard.scope + ')' }) }}"
          }
        }
      },
      "id": "log-approval",
      "name": "Log Approval Decision",
      "type": "n8n-nodes-base.executeWorkflow",
      "typeVersion": 1.2,
      "position": [5140, -100]
    },
    {
      "parameters": {
        "mode": "combine",
        "mergeByFields": {
          "values": []
        },
        "options": {}
      },
      "id": "merge-storage-results",
      "name": "Merge Storage Results",
      "type": "n8n-nodes-base.merge",
      "typeVersion": 3,
      "position": [5380, -100]
    },
    {
      "parameters": {
        "jsCode": "// Final aggregation of scavenging results\n\nconst allResults = $input.all();\nconst firstItem = allResults[0]?.json || {};\n\n// Count by scope\nlet approvedGlobal = 0;\nlet approvedLocal = 0;\n\nconst storedStandards = allResults.map(item => {\n  const std = item.json.current_standard;\n  if (std) {\n    if (std.scope === 'global') approvedGlobal++;\n    else if (std.scope === 'local') approvedLocal++;\n  }\n  return std;\n}).filter(Boolean);\n\nreturn {\n  json: {\n    project_id: firstItem.project_id,\n    session_id: firstItem.session_id,\n    scavenging_id: firstItem.scavenging_id,\n    phase: 0,\n    status: 'completed',\n    summary: {\n      total_standards_found: firstItem.total_processed || storedStandards.length,\n      approved_global: approvedGlobal,\n      approved_local: approvedLocal,\n      skipped: (firstItem.total_skipped || 0)\n    },\n    processed_standards: storedStandards,\n    completed_at: new Date().toISOString()\n  }\n};"
      },
      "id": "final-aggregation",
      "name": "Final Aggregation",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [5600, 0]
    },
    {
      "parameters": {
        "jsCode": "// Subworkflow Output - Return results to caller\n\nconst data = $input.first().json;\n\nreturn {\n  json: {\n    project_id: data.project_id,\n    session_id: data.session_id,\n    scavenging_id: data.scavenging_id,\n    phase: data.phase || 0,\n    status: data.status || 'completed',\n    summary: data.summary || {},\n    processed_standards: data.processed_standards || [],\n    completed_at: data.completed_at || new Date().toISOString()\n  }\n};"
      },
      "id": "output-node",
      "name": "Subworkflow Output",
      "type": "n8n-nodes-base.code",
      "typeVersion": 2,
      "position": [5820, 0]
    }
  ],
  "connections": {
    "Error Trigger": {
      "main": [
        [
          {
            "node": "Error Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Subworkflow Entry Point": {
      "main": [
        [
          {
            "node": "State Initializer",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "State Initializer": {
      "main": [
        [
          {
            "node": "Prepare Documents",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Documents": {
      "main": [
        [
          {
            "node": "Document Batch Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Document Batch Processor": {
      "main": [
        [
          {
            "node": "Prepare Single Document",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Aggregate All Standards",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Prepare Single Document": {
      "main": [
        [
          {
            "node": "Download from S3",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Download from S3": {
      "main": [
        [
          {
            "node": "Extract Text Content",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Extract Text Content": {
      "main": [
        [
          {
            "node": "Scavenger Agent",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Claude 3.5 Sonnet": {
      "ai_languageModel": [
        [
          {
            "node": "Scavenger Agent",
            "type": "ai_languageModel",
            "index": 0
          }
        ]
      ]
    },
    "Scavenger Agent": {
      "main": [
        [
          {
            "node": "Parse Extracted Standards",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Extracted Standards": {
      "main": [
        [
          {
            "node": "Document Batch Processor",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate All Standards": {
      "main": [
        [
          {
            "node": "Has Standards?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Standards?": {
      "main": [
        [
          {
            "node": "Split for Perplexity",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "No Standards Handler",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "No Standards Handler": {
      "main": [
        [
          {
            "node": "Subworkflow Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split for Perplexity": {
      "main": [
        [
          {
            "node": "Perplexity Batch (3 parallel)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Perplexity Batch (3 parallel)": {
      "main": [
        [
          {
            "node": "Fetch Alternatives (Perplexity)",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Aggregate Enriched Standards",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Fetch Alternatives (Perplexity)": {
      "main": [
        [
          {
            "node": "Parse Alternatives",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Parse Alternatives": {
      "main": [
        [
          {
            "node": "Perplexity Batch (3 parallel)",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Aggregate Enriched Standards": {
      "main": [
        [
          {
            "node": "Build Governance Payload",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Build Governance Payload": {
      "main": [
        [
          {
            "node": "Send Governance Message to Dashboard",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Send Governance Message to Dashboard": {
      "main": [
        [
          {
            "node": "Wait for Batch Governance Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Wait for Batch Governance Response": {
      "main": [
        [
          {
            "node": "Process Batch Response",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Process Batch Response": {
      "main": [
        [
          {
            "node": "Split Approved for Storage",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Split Approved for Storage": {
      "main": [
        [
          {
            "node": "Has Approved Standards?",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Has Approved Standards?": {
      "main": [
        [
          {
            "node": "Store in Graphiti",
            "type": "main",
            "index": 0
          },
          {
            "node": "Store in Qdrant",
            "type": "main",
            "index": 0
          }
        ],
        [
          {
            "node": "Final Aggregation",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store in Graphiti": {
      "main": [
        [
          {
            "node": "Log Approval Decision",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Store in Qdrant": {
      "main": [
        [
          {
            "node": "Log Approval Decision",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Log Approval Decision": {
      "main": [
        [
          {
            "node": "Merge Storage Results",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Merge Storage Results": {
      "main": [
        [
          {
            "node": "Final Aggregation",
            "type": "main",
            "index": 0
          }
        ]
      ]
    },
    "Final Aggregation": {
      "main": [
        [
          {
            "node": "Subworkflow Output",
            "type": "main",
            "index": 0
          }
        ]
      ]
    }
  },
  "active": false,
  "settings": {
    "executionOrder": "v1"
  },
  "versionId": "2",
  "meta": {
    "templateCredsSetupCompleted": true,
    "instanceId": "ai-product-factory"
  },
  "id": "ai-product-factory-scavenging",
  "tags": [
    {
      "id": "ai-product-factory",
      "name": "AI Product Factory"
    }
  ]
}
